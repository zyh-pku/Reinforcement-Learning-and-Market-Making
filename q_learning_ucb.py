

# -*- coding: utf-8 -*-
"""UCB_new_codes_to_upload.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OD5e9VAOroqtrans_prob_matrix_WdsmidpriceLJpzeyGfIpPTGO2
"""

import numpy as np
import matplotlib.pyplot as plt
from market import MarketEnvironment

class QLearningAgent:

    def __init__(self, dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, Delta,  \
        Q_upper_bound=4., UCB=True,\
        bonus_coef_0=0.1, bonus_coef_1=1., ucb_H=50, \
        eps = 0.8, eps0 = 0.95, eps_epoch = 2, \
        exp = 1.0, exp0 = 0.8, exp_epoch = 100, \
        N_RL_iter=12*10**4, N_learning_steps=3*10**4):
        
        self.dim_midprice_grid = dim_midprice_grid
        self.dim_inventory_grid = dim_inventory_grid
        self.dim_action_ask_price = dim_action_ask_price
        self.dim_action_buy_price = dim_action_buy_price
        self.N_RL_iter = N_RL_iter
        self.N_learning_steps = N_learning_steps
        self.Delta = Delta
        self.GAMMA = 0.95
        self.GAMMA_Delta = np.exp(-self.GAMMA*self.Delta)
        self.Q_upper_bound = Q_upper_bound
        # ucb
        self.UCB = UCB  # if True , use UCB exploration; if False, use eps-greedy exploration
        self.bonus_coef_0 = bonus_coef_0
        self.bonus_coef_1 = bonus_coef_1
        self.ucb_H = ucb_H

        # eps-greedy
        self.eps = eps
        self.eps0 = eps0
        self.eps_epoch = eps_epoch
        self.exp = exp
        self.exp0 = exp0
        self.exp_epoch = exp_epoch

        # Initialize Q-table and other matrices (for vanilla Q-learning with eps-greedy exploration)
        self.Q_table = np.zeros((dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price))
        self.Q_table_track = np.zeros((dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, N_RL_iter))
        self.state_counter_matrix = np.zeros((dim_midprice_grid, dim_inventory_grid))
        self.state_action_counter_matrix = np.zeros((dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price))

        # Initialize an additional Q_hat-table for Q-learning with UCB exploration
        self.Q_hat_table = np.zeros( (dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price) ) + self.Q_upper_bound
        self.Q_hat_table_track = np.zeros( (dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, N_RL_iter) ) + self.Q_upper_bound

        self.set_learning_rate()

        if self.UCB:
            # Define bonus rate (for Q-learning with UCB exploration)
            bonus_list = [np.sqrt( (self.bonus_coef_1 * np.log(i+2) + self.bonus_coef_0 )/(i+1) ) for i in range(N_learning_steps)]
            self.bonus_matrix = np.zeros((dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, N_learning_steps))
            for idx_midprice in range(dim_midprice_grid):
                for idx_inventory in range(dim_inventory_grid):
                    for idx_ask_price in range(dim_action_ask_price):
                        for idx_buy_price in range(dim_action_buy_price):
                            self.bonus_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price, :] = np.array(bonus_list)

        else:
            # Define exploration probability (for vanilla Q-learning with eps-greedy exploration)
            explore_epsilon_list = [self.exp*((self.exp0)**(i//self.exp_epoch)) for i in range(N_learning_steps)]
            self.explore_prob_matrix = np.zeros((dim_midprice_grid, dim_inventory_grid, N_learning_steps))
            for idx_midprice in range(dim_midprice_grid):
                for idx_inventory in range(dim_inventory_grid):
                    self.explore_prob_matrix[idx_midprice, idx_inventory, :] = np.array(explore_epsilon_list)

    def set_learning_rate(self, ):
        if self.UCB:
            # Define learning rate (for Q-learning with UCB exploration)
            learning_rate_schedule = [(self.ucb_H+1)/(self.ucb_H+i) for i in range(self.N_learning_steps)]
        else:
            # Define learning rate (for vanilla Q-learning with eps-greedy exploration)
            learning_rate_schedule = [self.eps*((self.eps0)**(i//self.eps_epoch)) for i in range(self.N_learning_steps)]

        self.learning_rate_matrix = np.zeros((dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, self.N_learning_steps))
        for idx_midprice in range(dim_midprice_grid):
            for idx_inventory in range(dim_inventory_grid):
                for idx_ask_price in range(dim_action_ask_price):
                    for idx_buy_price in range(dim_action_buy_price):
                        self.learning_rate_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price, :] = np.array(learning_rate_schedule)


    def update(self, env):
        if self.UCB:
            self.Q_table = np.zeros( (dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price) ) + self.Q_upper_bound

        for i in range(self.N_RL_iter):
            if i % (10**4) == 0:
                print(f"Iteration: {i}")
            # the transition from i to i+1
            self.Q_table_track[:,:,:,:,i] = self.Q_table[:,:,:,:]
            self.Q_hat_table_track[:,:,:,:,i] = self.Q_hat_table[:,:,:,:]
            # main part of the Q-learning algorithm

            # Part 1: choose the action to do given the state at i-th time point
            idx_ask_price = 0 # quoted ask price
            idx_buy_price = 0 # quoted bid price
            #########
            # Make a list of the actions available from the current state
            idx_midprice = int(env.midprice_data) # all are integers, i.e., 0,1,2,...,dim_midprice_grid-1
            idx_inventory = int(env.inventory_data) # all are integers, i.e., 0,1,2,...,dim_inventory_grid-1
            count_state = int(self.state_counter_matrix[idx_midprice, idx_inventory])

            self.state_counter_matrix[idx_midprice,idx_inventory] = count_state+1
            
            if not self.UCB:
                EPSILON = self.explore_prob_matrix[ idx_midprice, idx_inventory, count_state ]  #exploration probability for this state

            midprice_integer = idx_midprice + 1 # midprice=midprice_integer*tick_size/2 and midprice_integer is in (1,2,...,dim_midprice_grid)
            inventory = idx_inventory - env.bound_inventory # inventory = the true signed integer value of inventory

            if inventory == -env.bound_inventory: # then sell order is not allowed
                action_ask_price_list = [dim_action_ask_price-1] # do nothing for ask order
                action_buy_price_list = env.price_list[env.price_list<midprice_integer/2] # the action is exactly equal to the index
                # midprice_integer/2 is because the middle price is on grid: 0, 1/2, 1, 3/2, 2, ...,
                # but the quoted price is on grid: 0,1,2,...
                idx_ask_price = dim_action_ask_price-1

                if not self.UCB and np.random.binomial(1, EPSILON) == 1:
                    idx_buy_price = np.random.choice(action_buy_price_list)
                else:
                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( list(action_ask_price_list), list(action_buy_price_list) )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_buy_price = action_buy_price_list[ idx_optimal_buy[0] ]

            elif inventory == env.bound_inventory: # then buy order is not allowed
                action_ask_price_list = env.price_list[env.price_list>midprice_integer/2]
                action_buy_price_list = [dim_action_buy_price-1] # do nothing for buy order
                idx_buy_price = dim_action_buy_price-1

                if not self.UCB and np.random.binomial(1, EPSILON) == 1:
                    idx_ask_price = np.random.choice(action_ask_price_list)
                else:
                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( list(action_ask_price_list), list(action_buy_price_list) )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_ask_price = action_ask_price_list[ idx_optimal_ask[0] ]


            else: # then both sell and buy orders are allowed
                action_ask_price_list = env.price_list[env.price_list>midprice_integer/2] # the action is exactly equal to the index
                action_buy_price_list = env.price_list[env.price_list<midprice_integer/2] # the action is exactly equal to the index
                if not self.UCB and np.random.binomial(1, EPSILON) == 1:
                    idx_ask_price = np.random.choice(action_ask_price_list)
                    idx_buy_price = np.random.choice(action_buy_price_list)
                else:
                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( list(action_ask_price_list), list(action_buy_price_list) )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_ask_price = action_ask_price_list[ idx_optimal_ask[0] ] # Wrong(do not do this!!! the index list is different!!): indeed, we can directly use idx_optimal_buy[0], because the action is exactly equal to the index
                    idx_buy_price = action_buy_price_list[ idx_optimal_buy[0] ]

            # the above will output idx_ask_price idx_buy_price, this action is together with the state at time i

            # then we update our counter for (state, action) pair
            count_state_action = int(self.state_action_counter_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price])
            self.state_action_counter_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price] = count_state_action+1


            # observe the reward and the next state

            reward, idx_midprice_next, idx_inventory_next, action_ask_price_list, action_buy_price_list = env.step(idx_ask_price, idx_buy_price, midprice_integer, inventory)

            Q_values_at_state = self.Q_table[idx_midprice_next, idx_inventory_next, :, :][np.ix_( action_ask_price_list, action_buy_price_list )]

            # Update Q-table
            if self.UCB:

                Q_value_new = reward + self.GAMMA_Delta * Q_values_at_state.max() + self.bonus_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price, count_state_action]
                Q_value_old = self.Q_hat_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price]

                self.Q_hat_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price] = self.learning_rate_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price, count_state_action] * (Q_value_new-Q_value_old) + Q_value_old

                self.Q_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price] = min(self.Q_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price], self.Q_hat_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price])

            else:

                Q_value_new = reward + self.GAMMA_Delta * Q_values_at_state.max()
                Q_value_old = self.Q_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price]

                self.Q_table[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price] = self.learning_rate_matrix[idx_midprice, idx_inventory, idx_ask_price, idx_buy_price, count_state_action] * (Q_value_new-Q_value_old) + Q_value_old


        self.plot_result()

    def plot_result(self,):
        plt.figure(1, figsize=(20, 8))
        M=self.N_RL_iter
        plt.plot(self.Q_table_track[0,2,1,3,:M], label='a=1')
        plt.plot(self.Q_table_track[0,2,2,3,:M], label='a=2')
        plt.plot(self.Q_table_track[0,2,0,3,:M])
        plt.plot(self.Q_table_track[0,2,3,3,:M])
        plt.xlabel('step')
        plt.ylabel('Q(s,a)')
        plt.show()

        if self.UCB:
            plt.figure(2, figsize=(20, 8))
            M=self.N_RL_iter
            plt.plot(self.Q_hat_table_track[0,2,1,3,:M], label='a=1')
            plt.plot(self.Q_hat_table_track[0,2,2,3,:M], label='a=2')
            plt.plot(self.Q_hat_table_track[0,2,0,3,:M])
            plt.plot(self.Q_hat_table_track[0,2,3,3,:M])
            plt.xlabel('step')
            plt.ylabel('Q_hat(s,a)')
            plt.show()

    def plot_learning_parameters(self):
        if self.UCB:
            label_prefix = 'UCB'
        else:
            label_prefix = 'epsilon-greedy'
        # Plot learning rate
        plt.figure(1, figsize=(16, 5))
        plt.plot(self.learning_rate_matrix[0, 0, 0, 0, :], label=f'{label_prefix}: learning rate')
        plt.xlabel('step')
        plt.ylabel('Learning Rate')
        plt.show()

        if self.UCB:
            # Plot bonus
            plt.figure(2, figsize=(16, 5))
            plt.plot(self.bonus_matrix[0, 0, 0, 0, :], label='bonus')
            plt.xlabel('step')
            plt.ylabel('Bonus')
            plt.show()
        else:
            # Plot exploration probability
            plt.figure(2, figsize=(16, 5))
            plt.plot(self.explore_prob_matrix[0, 0, :], label='exploration probability')
            plt.xlabel('step')
            plt.ylabel('Probability for Exploration')
            plt.show()

    def results_check(self, ):
        print('---------- the visiting number for each state: ----------')
        print(self.state_counter_matrix)
        action_ask_price_RL = np.zeros( (dim_midprice_grid, dim_inventory_grid) )
        action_buy_price_RL = np.zeros( (dim_midprice_grid, dim_inventory_grid) )
        V_RL = np.zeros( (dim_midprice_grid, dim_inventory_grid) )
        print('---------- the Q function for each state: ----------')
        for idx_midprice in range(dim_midprice_grid):
            for idx_inventory in range(dim_inventory_grid):
                midprice_integer = int(idx_midprice + 1)
                inventory = int(idx_inventory - env.bound_inventory)
                if inventory == -env.bound_inventory: # then sell order is not allowed
                    action_ask_price_list = [dim_action_ask_price-1] # do nothing for ask order
                    action_buy_price_list = env.price_list[env.price_list<midprice_integer/2] # the action is exactly equal to the index
                    print( f'(midprice_integer,inventory)={midprice_integer},{inventory}' )
                    print(action_ask_price_list)
                    print(action_buy_price_list)
                    #print( state_action_counter_matrix[ idx_midprice, idx_inventory, action_ask_price_list, action_buy_price_list] )
                    print( self.state_action_counter_matrix[ idx_midprice, idx_inventory, :, :] )
                    print( self.Q_table[ idx_midprice, idx_inventory, :, :] )

                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( action_ask_price_list, action_buy_price_list )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_ask_price = dim_action_ask_price-1
                    idx_buy_price = action_buy_price_list[ idx_optimal_buy[0] ]

                elif inventory == env.bound_inventory: # then buy order is not allowed
                    action_ask_price_list = env.price_list[env.price_list>midprice_integer/2]
                    action_buy_price_list = [dim_action_buy_price-1] # do nothing for buy order
                    print( f'(midprice_integer,inventory)={midprice_integer},{inventory}' )
                    print(action_ask_price_list)
                    print(action_buy_price_list)
                    #print( state_action_counter_matrix[ idx_midprice, idx_inventory, action_ask_price_list, action_buy_price_list] )
                    print( self.state_action_counter_matrix[ idx_midprice, idx_inventory, :, :] )
                    print( self.Q_table[ idx_midprice, idx_inventory, :, :] )

                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( action_ask_price_list, action_buy_price_list )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_ask_price = action_ask_price_list[ idx_optimal_ask[0] ]
                    idx_buy_price = dim_action_buy_price-1

                else: # then both sell and buy orders are allowed
                    action_ask_price_list = env.price_list[env.price_list>midprice_integer/2] # the action is exactly equal to the index
                    action_buy_price_list = env.price_list[env.price_list<midprice_integer/2] # the action is exactly equal to the index
                    print( f'(midprice_integer,inventory)={midprice_integer},{inventory}' )
                    print(action_ask_price_list)
                    print(action_buy_price_list)
                    #print( state_action_counter_matrix[ idx_midprice, idx_inventory, action_ask_price_list, action_buy_price_list] )
                    print( self.state_action_counter_matrix[ idx_midprice, idx_inventory, :, :] )
                    print( self.Q_table[ idx_midprice, idx_inventory, :, :] )

                    Q_values_at_state = self.Q_table[idx_midprice, idx_inventory, :, :][np.ix_( action_ask_price_list, action_buy_price_list )]
                    idx_optimal_ask, idx_optimal_buy = np.where(Q_values_at_state == Q_values_at_state.max())
                    idx_ask_price = action_ask_price_list[ idx_optimal_ask[0] ]
                    idx_buy_price = action_buy_price_list[ idx_optimal_buy[0] ]

                V_RL[idx_midprice, idx_inventory] = Q_values_at_state.max()
                action_ask_price_RL[idx_midprice, idx_inventory] = idx_ask_price
                action_buy_price_RL[idx_midprice, idx_inventory] = idx_buy_price
        print('---------- the learned value function and policy: ----------')
        print(V_RL)
        print(action_ask_price_RL)
        print(action_buy_price_RL)

if __name__ == "__main__":

    # Example usage
    dim_price_grid = 3 # N_P: price grid dimension - 1 (because we start from 0)
    bound_inventory = 2 # N_Y: (inventory grid dimension - 1)/2 (because we allow both - and + and 0)
    Delta = 0.1

    dim_midprice_grid = 2*dim_price_grid-1
    dim_inventory_grid = 2*bound_inventory+1
    dim_action_ask_price = dim_price_grid+2
    dim_action_buy_price = dim_price_grid+2

    """##UCB"""

    agent = QLearningAgent(dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, Delta,
                           UCB=True)#, bonus_coef_0=0.1,  bonus_coef_1=1., ucb_H=5, Q_upper_bound=3.8)
    agent.plot_learning_parameters()

    env = MarketEnvironment(dim_price_grid, bound_inventory, dim_action_ask_price, dim_action_buy_price, Delta)
    env.reset()

    np.random.seed(999)

    agent.update(env)

    agent.results_check()

    """## eps-greedy"""

    agent = QLearningAgent(dim_midprice_grid, dim_inventory_grid, dim_action_ask_price, dim_action_buy_price, Delta, 
                           UCB=False, N_RL_iter=6*10**3, N_learning_steps=3*10**3)
    agent.plot_learning_parameters()

    env = MarketEnvironment(dim_price_grid, bound_inventory, dim_action_ask_price, dim_action_buy_price, Delta)
    env.reset()

    np.random.seed(999)

    agent.update(env)

    agent.results_check()