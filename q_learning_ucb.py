# -*- coding: utf-8 -*-
"""UCB_new_codes_to_upload.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OD5e9VAOroqP_WdsXLJpzeyGfIpPTGO2
"""

import numpy as np
import matplotlib.pyplot as plt
from market import MarketEnvironment

class QLearningAgent:

    def __init__(self, dim_X, dim_Y, dim_action_a, dim_action_b, Delta,  \
        Q_upper_bound=4., UCB=True,\
        bonus_coef_0=0., bonus_coef_1=0., ucb_H=50, \
        eps = 0.8, eps0 = 0.95, eps_epoch = 2, \
        exp = 1.0, exp0 = 0.8, exp_epoch = 100, \
        N_RL_iter=10**5, N_learning_steps=3*10**4):
        self.dim_X = dim_X
        self.dim_Y = dim_Y
        self.dim_action_a = dim_action_a
        self.dim_action_b = dim_action_b
        self.N_RL_iter = N_RL_iter
        self.N_learning_steps = N_learning_steps
        self.Delta = Delta
        self.GAMMA = 0.95
        self.GAMMA_Delta = np.exp(-self.GAMMA*self.Delta)
        self.Q_upper_bound = Q_upper_bound
        # ucb
        self.UCB = UCB  # if True , use UCB exploration; if False, use eps-greedy exploration
        self.bonus_coef_0 = bonus_coef_0
        self.bonus_coef_1 = bonus_coef_1
        self.ucb_H = ucb_H

        # eps-greedy
        self.eps = eps
        self.eps0 = eps0
        self.eps_epoch = eps_epoch
        self.exp = exp
        self.exp0 = exp0
        self.exp_epoch = exp_epoch

        # Initialize Q-table and other matrices (for vanilla Q-learning with eps-greedy exploration)
        self.Q_table = np.zeros((dim_X, dim_Y, dim_action_a, dim_action_b))
        self.Q_table_track = np.zeros((dim_X, dim_Y, dim_action_a, dim_action_b, N_RL_iter))
        self.state_counter_matrix = np.zeros((dim_X, dim_Y))
        self.state_action_counter_matrix = np.zeros((dim_X, dim_Y, dim_action_a, dim_action_b))

        # Initialize an additional Q_hat-table for Q-learning with UCB exploration
        self.Q_hat_table = np.zeros( (dim_X, dim_Y, dim_action_a, dim_action_b) ) + self.Q_upper_bound
        self.Q_hat_table_track = np.zeros( (dim_X, dim_Y, dim_action_a, dim_action_b, N_RL_iter) ) + self.Q_upper_bound

        self.set_learning_rate()

        if self.UCB:
            # Define bonus rate (for Q-learning with UCB exploration)
            bonus_list = [np.sqrt( (self.bonus_coef_1 * np.log(i+2) + self.bonus_coef_0 )/(i+1) ) for i in range(N_learning_steps)]
            self.bonus_matrix = np.zeros((dim_X, dim_Y, dim_action_a, dim_action_b, N_learning_steps))
            for idx_x in range(dim_X):
                for idx_y in range(dim_Y):
                    for p_a in range(dim_action_a):
                        for p_b in range(dim_action_b):
                            self.bonus_matrix[idx_x, idx_y, p_a, p_b, :] = np.array(bonus_list)

        else:
            # Define exploration probability (for vanilla Q-learning with eps-greedy exploration)
            explore_epsilon_list = [self.exp*((self.exp0)**(i//self.exp_epoch)) for i in range(N_learning_steps)]
            self.explore_prob_matrix = np.zeros((dim_X, dim_Y, N_learning_steps))
            for idx_x in range(dim_X):
                for idx_y in range(dim_Y):
                    self.explore_prob_matrix[idx_x, idx_y, :] = np.array(explore_epsilon_list)

    def set_learning_rate(self, ):
        if self.UCB:
            # Define learning rate (for Q-learning with UCB exploration)
            learning_rate_schedule = [(self.ucb_H+1)/(self.ucb_H+i) for i in range(self.N_learning_steps)]
        else:
            # Define learning rate (for vanilla Q-learning with eps-greedy exploration)
            learning_rate_schedule = [self.eps*((self.eps0)**(i//self.eps_epoch)) for i in range(self.N_learning_steps)]

        self.learning_rate_matrix = np.zeros((dim_X, dim_Y, dim_action_a, dim_action_b, self.N_learning_steps))
        for idx_x in range(dim_X):
            for idx_y in range(dim_Y):
                for p_a in range(dim_action_a):
                    for p_b in range(dim_action_b):
                        self.learning_rate_matrix[idx_x, idx_y, p_a, p_b, :] = np.array(learning_rate_schedule)


    def update(self, env):
        if self.UCB:
            self.Q_table = np.zeros( (dim_X, dim_Y, dim_action_a, dim_action_b) ) + self.Q_upper_bound

        for i in range(self.N_RL_iter):
            if i % (10**4) == 0:
                print(f"Iteration: {i}")
            # the transition from i to i+1
            self.Q_table_track[:,:,:,:,i] = self.Q_table[:,:,:,:]
            self.Q_hat_table_track[:,:,:,:,i] = self.Q_hat_table[:,:,:,:]
            # main part of the Q-learning algorithm

            # Part 1: choose the action to do given the state at i-th time point
            p_a = 0 # quoted ask price
            p_b = 0 # quoted bid price
            #########
            # Make a list of the actions available from the current state
            idx_x = int(env.X_data) # all are integers, i.e., 0,1,2,...,dim_X-1
            idx_y = int(env.Y_data) # all are integers, i.e., 0,1,2,...,dim_Y-1

            count_xy = int(self.state_counter_matrix[idx_x,idx_y])
            self.state_counter_matrix[idx_x,idx_y] = count_xy+1

            EPSILON = self.explore_prob_matrix[ idx_x, idx_y, count_xy ]  #exploration probability for this state

            x = idx_x + 1 # midprice=x*tick_size/2 and x is in (1,2,...,dim_X)
            y = idx_y - env.N_Y # y = the true signed integer value of inventory

            if y == -env.N_Y: # then sell order is not allowed
                action_a_list = [dim_action_a-1] # do nothing for ask order
                action_b_list = env.prices[env.prices<x/2] # the action is exactly equal to the index
                # x/2 is because the middle price is on grid: 0, 1/2, 1, 3/2, 2, ...,
                # but the quoted price is on grid: 0,1,2,...
                p_a = dim_action_a-1

                if np.random.binomial(1, EPSILON) == 1 and not self.UCB:
                    p_b = np.random.choice(action_b_list)
                else:
                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( list(action_a_list), list(action_b_list) )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_b = action_b_list[ idx_max_b[0] ]

            elif y == env.N_Y: # then buy order is not allowed
                action_a_list = env.prices[env.prices>x/2]
                action_b_list = [dim_action_b-1] # do nothing for buy order
                p_b = dim_action_b-1

                if np.random.binomial(1, EPSILON) == 1 and not self.UCB:
                    p_a = np.random.choice(action_a_list)
                else:
                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( list(action_a_list), list(action_b_list) )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_a = action_a_list[ idx_max_a[0] ]


            else: # then both sell and buy orders are allowed
                action_a_list = env.prices[env.prices>x/2] # the action is exactly equal to the index
                action_b_list = env.prices[env.prices<x/2] # the action is exactly equal to the index
                if np.random.binomial(1, EPSILON) == 1 and not self.UCB:
                    p_a = np.random.choice(action_a_list)
                    p_b = np.random.choice(action_b_list)
                else:
                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( list(action_a_list), list(action_b_list) )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_a = action_a_list[ idx_max_a[0] ] # Wrong(do not do this!!! the index list is different!!): indeed, we can directly use idx_max_b[0], because the action is exactly equal to the index
                    p_b = action_b_list[ idx_max_b[0] ]

            # the above will output p_a p_b, this action is together with the state at time i

            # then we update our counter for (state, action) pair
            count_s_a = int(self.state_action_counter_matrix[idx_x, idx_y, p_a, p_b])
            self.state_action_counter_matrix[idx_x, idx_y, p_a, p_b] = count_s_a+1


            # observe the reward and the next state

            reward, idx_x_i1, idx_y_i1, action_a_list, action_b_list = env.step(p_a, p_b, x, y, idx_x)

            Q_values_xy = self.Q_table[idx_x_i1, idx_y_i1, :, :][np.ix_( action_a_list, action_b_list )]

            Q_value_max_new_i1 = Q_values_xy.max()

            # Update Q-table
            if self.UCB:

                Q_value_new = reward + self.GAMMA_Delta * Q_value_max_new_i1 + self.bonus_matrix[idx_x, idx_y, p_a, p_b, count_s_a]
                Q_value_old = self.Q_hat_table[idx_x, idx_y, p_a, p_b]

                self.Q_hat_table[idx_x, idx_y, p_a, p_b] = self.learning_rate_matrix[idx_x, idx_y, p_a, p_b, count_s_a] * (Q_value_new-Q_value_old) + Q_value_old

                self.Q_table[idx_x, idx_y, p_a, p_b] = min(self.Q_table[idx_x, idx_y, p_a, p_b], self.Q_hat_table[idx_x, idx_y, p_a, p_b])

            else:

                Q_value_new = reward + self.GAMMA_Delta * Q_value_max_new_i1
                Q_value_old = self.Q_table[idx_x, idx_y, p_a, p_b]

                self.Q_table[idx_x, idx_y, p_a, p_b] = self.learning_rate_matrix[idx_x, idx_y, p_a, p_b, count_s_a] * (Q_value_new-Q_value_old) + Q_value_old


        self.plot_result()

    def plot_result(self,):
        plt.figure(1, figsize=(20, 8))
        M=self.N_RL_iter
        plt.plot(self.Q_table_track[0,2,1,3,:M], label='a=1')
        plt.plot(self.Q_table_track[0,2,2,3,:M], label='a=2')
        plt.plot(self.Q_table_track[0,2,0,3,:M])
        plt.plot(self.Q_table_track[0,2,3,3,:M])
        plt.xlabel('step')
        plt.ylabel('Q(s,a)')
        plt.show()

        if self.UCB:
            plt.figure(2, figsize=(20, 8))
            M=self.N_RL_iter
            plt.plot(self.Q_hat_table_track[0,2,1,3,:M], label='a=1')
            plt.plot(self.Q_hat_table_track[0,2,2,3,:M], label='a=2')
            plt.plot(self.Q_hat_table_track[0,2,0,3,:M])
            plt.plot(self.Q_hat_table_track[0,2,3,3,:M])
            plt.xlabel('step')
            plt.ylabel('Q_hat(s,a)')
            plt.show()

    def plot_learning_parameters(self):
        if self.UCB:
            label_prefix = 'UCB'
        else:
            label_prefix = 'epsilon-greedy'
        # Plot learning rate
        plt.figure(1, figsize=(16, 5))
        plt.plot(self.learning_rate_matrix[0, 0, 0, 0, :], label=f'{label_prefix}: learning rate')
        plt.xlabel('step')
        plt.ylabel('Learning Rate')
        plt.show()

        if self.UCB:
            # Plot bonus
            plt.figure(2, figsize=(16, 5))
            plt.plot(self.bonus_matrix[0, 0, 0, 0, :], label='bonus')
            plt.xlabel('step')
            plt.ylabel('Bonus')
            plt.show()
        else:
            # Plot exploration probability
            plt.figure(2, figsize=(16, 5))
            plt.plot(self.explore_prob_matrix[0, 0, :], label='exploration probability')
            plt.xlabel('step')
            plt.ylabel('Probability for Exploration')
            plt.show()

    def results_check(self, ):
        print('---------- the visiting number for each state: ----------')
        print(self.state_counter_matrix)
        action_a_RL = np.zeros( (dim_X, dim_Y) )
        action_b_RL = np.zeros( (dim_X, dim_Y) )
        V_RL = np.zeros( (dim_X, dim_Y) )
        print('---------- the Q function for each state: ----------')
        for idx_x in range(dim_X):
            for idx_y in range(dim_Y):
                x = int(idx_x + 1)
                y = int(idx_y - env.N_Y)
                if y == -env.N_Y: # then sell order is not allowed
                    action_a_list = [dim_action_a-1] # do nothing for ask order
                    action_b_list = env.prices[env.prices<x/2] # the action is exactly equal to the index
                    print( f'(x,y)={x},{y}' )
                    print(action_a_list)
                    print(action_b_list)
                    #print( state_action_counter_matrix[ idx_x, idx_y, action_a_list, action_b_list] )
                    print( self.state_action_counter_matrix[ idx_x, idx_y, :, :] )
                    print( self.Q_table[ idx_x, idx_y, :, :] )

                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( action_a_list, action_b_list )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_a = dim_action_a-1
                    p_b = action_b_list[ idx_max_b[0] ]

                elif y == env.N_Y: # then buy order is not allowed
                    action_a_list = env.prices[env.prices>x/2]
                    action_b_list = [dim_action_b-1] # do nothing for buy order
                    print( f'(x,y)={x},{y}' )
                    print(action_a_list)
                    print(action_b_list)
                    #print( state_action_counter_matrix[ idx_x, idx_y, action_a_list, action_b_list] )
                    print( self.state_action_counter_matrix[ idx_x, idx_y, :, :] )
                    print( self.Q_table[ idx_x, idx_y, :, :] )

                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( action_a_list, action_b_list )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_a = action_a_list[ idx_max_a[0] ]
                    p_b = dim_action_b-1

                else: # then both sell and buy orders are allowed
                    action_a_list = env.prices[env.prices>x/2] # the action is exactly equal to the index
                    action_b_list = env.prices[env.prices<x/2] # the action is exactly equal to the index
                    print( f'(x,y)={x},{y}' )
                    print(action_a_list)
                    print(action_b_list)
                    #print( state_action_counter_matrix[ idx_x, idx_y, action_a_list, action_b_list] )
                    print( self.state_action_counter_matrix[ idx_x, idx_y, :, :] )
                    print( self.Q_table[ idx_x, idx_y, :, :] )

                    Q_values_xy = self.Q_table[idx_x, idx_y, :, :][np.ix_( action_a_list, action_b_list )]
                    idx_max_a, idx_max_b = np.where(Q_values_xy == Q_values_xy.max())
                    p_a = action_a_list[ idx_max_a[0] ]
                    p_b = action_b_list[ idx_max_b[0] ]

                V_RL[idx_x, idx_y] = Q_values_xy.max()
                action_a_RL[idx_x, idx_y] = p_a
                action_b_RL[idx_x, idx_y] = p_b
        print('---------- the learned value function and policy: ----------')
        print(V_RL)
        print(action_a_RL)
        print(action_b_RL)

if __name__ == "__main__":

    # Example usage
    N_P = 2 # price grid dimension - 1 (because we start from 0)
    N_Y = 1 # (inventory grid dimension - 1)/2 (because we allow both - and + and 0)
    Delta = 0.1

    dim_X = 2*N_P-1
    dim_Y = 2*N_Y+1
    dim_action_a = N_P+2
    dim_action_b = N_P+2

    """##UCB"""

    agent = QLearningAgent(dim_X, dim_Y, dim_action_a, dim_action_b, Delta, UCB=True, bonus_coef_0=0.1,  bonus_coef_1=1.)
    agent.plot_learning_parameters()

    env = MarketEnvironment(N_P, N_Y, dim_action_a, dim_action_b, Delta)
    env.reset()

    np.random.seed(999)

    agent.update(env)

    agent.results_check()

    """## eps-greedy"""

    agent = QLearningAgent(dim_X, dim_Y, dim_action_a, dim_action_b, Delta, UCB=False)
    agent.plot_learning_parameters()

    env = MarketEnvironment(N_P, N_Y, dim_action_a, dim_action_b, Delta)
    env.reset()

    np.random.seed(999)

    agent.update(env)

    agent.results_check()